{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import gensim\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "documents = fetch_20newsgroups(subset='train', remove=('headers', 'footers',  'quotes'), shuffle=True)\n",
    "\n",
    "# Preprocess with gensim and remove stopwords\n",
    "documents = [[word for word in gensim.utils.simple_preprocess(document) if not word in gensim.parsing.preprocessing.STOPWORDS]\n",
    "             for document in documents['data'][:100]]\n",
    "\n",
    "# Remove rare words by applying a frequency threshold\n",
    "freq_threshold = 5\n",
    "counts = Counter([word for document in documents for word in document])\n",
    "\n",
    "documents = [list(filter(lambda word : counts[word] > freq_threshold, document)) for document in documents]\n",
    "documents = list(filter(lambda document : len(document) > 0, documents))\n",
    "\n",
    "# Map words to integer indices\n",
    "counts = Counter([word for document in documents for word in document])\n",
    "\n",
    "word_idx_dict = {word : i for i, word in enumerate(counts.keys())}\n",
    "idx_word_dict = {i : word for word, i in word_idx_dict.items()}\n",
    "\n",
    "documents = [torch.tensor(list(map(lambda word : word_idx_dict[word], document))) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalLDA(nn.Module):\n",
    "    \n",
    "    def __init__(self, documents, vocab_size, num_topics, log_eta, log_alpha):\n",
    "        \n",
    "        super(VariationalLDA, self).__init__()\n",
    "        \n",
    "        self.D = len(documents)\n",
    "        self.K = num_topics\n",
    "        self.V = vocab_size\n",
    "        \n",
    "        self.words = documents\n",
    "        \n",
    "        k_ones = torch.ones(size=(self.K,)).to(device)\n",
    "        \n",
    "        self.log_eta = nn.Parameter(torch.tensor(log_eta).to(device))\n",
    "        self.log_alpha = nn.Parameter((log_alpha * k_ones + torch.rand(self.K).to(device)).to(device))\n",
    "        \n",
    "        self.gamma = [torch.exp(log_alpha * k_ones) + k_ones * len(Counter(document)) / self.K\n",
    "                      for document in documents]\n",
    "        \n",
    "        self.phi = [torch.ones(size=(document.shape[0], self.K)).to(device) / self.K\n",
    "                    for document in documents]\n",
    "        \n",
    "        self.lamda = torch.ones(size=(self.K, self.V)).to(device)\n",
    "    \n",
    "    \n",
    "    def elbo(self):\n",
    "        \n",
    "        self.eta = torch.exp(self.log_eta)\n",
    "        self.alpha = torch.exp(self.log_alpha)\n",
    "        \n",
    "        elbo = 0\n",
    "        \n",
    "        eta_vector = self.eta * torch.ones(self.V).to(device)\n",
    "        \n",
    "        lamda_digamma_diff = self.digamma_difference(self.lamda)\n",
    "        gamma_digamma_diff = [self.digamma_difference(gamma_d) for gamma_d in self.gamma]\n",
    "        \n",
    "        eta_log_gamma_diff = self.log_gamma_difference(eta_vector)\n",
    "        alpha_log_gamma_diff = self.log_gamma_difference(self.alpha)\n",
    "        gamma_log_gamma_diff = [self.log_gamma_difference(gamma_d) for gamma_d in self.gamma]\n",
    "        lamda_log_gamma_diff = self.log_gamma_difference(self.lamda)\n",
    "        \n",
    "        elbo = elbo + sum([torch.einsum('nk, kn ->', phi_d, lamda_digamma_diff[:, words_d])\n",
    "                           for phi_d, words_d in zip(self.phi, self.words)])\n",
    "        \n",
    "        elbo = elbo + sum([torch.einsum('nk, k ->', phi_d, gamma_digamma_diff_d)\n",
    "                           for phi_d, gamma_digamma_diff_d in zip(self.phi, gamma_digamma_diff)])\n",
    "        \n",
    "        elbo = elbo + self.D * alpha_log_gamma_diff\n",
    "        \n",
    "        elbo = elbo + sum([torch.einsum('k, k ->', self.alpha - 1, gamma_digamma_diff_d)\n",
    "                           for gamma_digamma_diff_d in gamma_digamma_diff])\n",
    "        \n",
    "        elbo = elbo + self.K * eta_log_gamma_diff\n",
    "        \n",
    "        elbo = elbo + torch.einsum('v, kv ->', eta_vector - 1, lamda_digamma_diff)\n",
    "        \n",
    "        elbo = elbo - sum([torch.sum(phi_d * torch.log(phi_d)) for phi_d in self.phi])\n",
    "        \n",
    "        elbo = elbo - sum(gamma_log_gamma_diff)\n",
    "        \n",
    "        elbo = elbo - sum([torch.einsum('k, k ->', gamma_d - 1, gamma_digamma_diff_d)\n",
    "                           for gamma_d, gamma_digamma_diff_d in zip(self.gamma, gamma_digamma_diff)])\n",
    "        \n",
    "        elbo = elbo - torch.sum(lamda_log_gamma_diff)\n",
    "        \n",
    "        elbo = elbo - torch.einsum('kv, kv ->', self.lamda - 1, lamda_digamma_diff)\n",
    "        \n",
    "        return elbo\n",
    "    \n",
    "    \n",
    "    def forward(self):\n",
    "        return - self.elbo()\n",
    "    \n",
    "    \n",
    "    def variational_parameter_step(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def model_parameter_step(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def perplexity(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def digamma_difference(self, tensor):\n",
    "        return torch.digamma(tensor) - torch.digamma(torch.sum(tensor, dim=-1)[..., None])\n",
    "    \n",
    "    \n",
    "    def log_gamma_difference(self, tensor):\n",
    "        return torch.lgamma(torch.sum(tensor, dim=-1)) - torch.sum(torch.lgamma(tensor), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = VariationalLDA(documents, len(counts), num_topics=5, log_eta=0., log_alpha=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[1.0951843 2.1340022 2.1017673 1.1532717 1.3771474]\n",
      "1.0\n",
      "[1.1744578 2.0392492 2.0164607 1.230205  1.4367872]\n",
      "1.0\n",
      "[1.2514659 1.9743441 1.9577085 1.3039268 1.4912012]\n",
      "1.0\n",
      "[1.3257719 1.9308741 1.9184316 1.3742497 1.5414073]\n",
      "1.0\n",
      "[1.3970369 1.9033227 1.8938441 1.4410876 1.5882629]\n",
      "1.0\n",
      "[1.4650337 1.8879035 1.8805801 1.504446  1.6324649]\n",
      "1.0\n",
      "[1.5296443 1.8819125 1.8761939 1.564409  1.6745648]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(lda.parameters(), lr=1e-5)\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(torch.exp(lda.log_eta).cpu().detach().numpy())\n",
    "        \n",
    "        print(torch.exp(lda.log_alpha).cpu().detach().numpy())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    neg_elbo = lda.forward()\n",
    "\n",
    "    neg_elbo.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-lda",
   "language": "python",
   "name": "venv-lda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
